---
layout: post
title: "评测机制"
author: "赵佳明"
categories: internship
tags: [confidential]
published: false
image: internship-cover-3.png

---

## 1.创建评测集

上传一个excel表格，包含五列

- 用户问题(Question)
  必填，用户向应用提问的说法
  eg:你今天吃了什么
- 标准答案(Ground Truths)
  必填，问题的标准答案，尽量与知识库说法一致，如果有多个请用"||"隔开
  eg:我还不饿||还没吃饭呢
- 兜底回复
  必填，大模型无法回答时给出的兜底话术，如果有多个请用"||"隔开
  eg:抱歉，我没有听懂你的问题哦，但我会努力学习，不断进步的～～||你好，这个问题我还在学习中呢，您可以和我描述下具到的具体情况，或者也可以寻求人工支持。
- 召回上下文(Contexts)
  非必填，可以在会话日志直接导出，自行上传可参考格式{"retrievedDoc": [{"text": "aaa","textId": "12345","score":0.1}],"retrievedFaq": [{"answer": "bbbb","question":"你好？","questionId": "12346","score":0.2}]}
  eg:{"retrievedDoc": [{"text": "我还不饿","textId": "12345","score":0.99}],"retrievedFaq": [{"answer": "还没吃饭呢","question":"你今天吃了什么？","questionId": "12346","score":0.99}]}
- 模型回答(Answer)
  非必填，可以在会话日志直接导出
  eg:我还不饿



## 2.评测指标

包含三个方面

###  1.召回

- 知识有召回率
  召回阶段有检索到知识的比例。

- 知识命中率
  有有召回上下文的查询中，其召回上下文整体包含正确答案的比例。

- 上下文召回率（需大模型参与）
  Context Recall（上下文召回率）是一个衡量检索到的上下文与标注答案（即真实答案）一致程度的指标。这个指标的值在0到1之间，数值越高说明召回上下文涵盖正确答案的程度越高。 该指标业界叫“召回率”（Recall = Number of Relevant ltems Retrieved / Total Number of Relevant ltems），分子和分母的计算方法一般依赖人工标签或专家评估或基于用户行为反馈；基于GT的算法可以理解成一种变体。

- 上下文精确率（需大模型参与）
  评估在召回上下文中所有与基本事实相关的片段是否都被排名较高。理想情况下，相关切片在召回切片中得分排名最靠前。 该指标可以对标业界的NDCG，通过检索片段的相关性和排名衡量检索质量。检索到的文本块的排名在token超过4K时尤为重要，因为LLM存在“lost in the middle”现象，4k上下文能满足绝大部分的用户问题。

- 召回知识冲突率
  召回上下文中，存在问题相似但答案不一致的知识，视为矛盾冲突。统计在召回环节，存在冲突的比例。

### 2.知识库

- 知识覆盖率
  测评集中基本事实（ground truth）对应的知识在应用绑定知识库中存在的比例

### 3.端到端

- 正确拒答率（需大模型参与）
  标准问题或正确答案不在知识库中，且LLM输出兜底文案的比例。

- 错误拒答率（需大模型参与）
  标准问题或正确答案在知识库中，但LLM输出兜底文案的比例

- 答案准确率_相似度
  输出答案与标准答案(ground truth)的相似度，超过阈值则认为输出答案准确。

- 答案准确率_大模型（需大模型参与）
  对于大模型输出的答案，通过新的大模型查询判断其与标准答案(ground truth)在含义上是否完全一致，无冗余、无缺漏。

- 答案缺漏率（需大模型参与）
  LLM输出答案都存在于召回上下文中，无编造的冗余内容；输出答案与标准答案相比，正确但不全。
